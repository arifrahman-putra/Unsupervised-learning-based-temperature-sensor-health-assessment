{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f888222-aa19-40fa-b337-1f311c05ae75",
   "metadata": {},
   "source": [
    "## Acquire raw temperature dataset from a local server PostgreSQL table using psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b441ee3",
   "metadata": {},
   "source": [
    "table name = \"temperature_sensor_recording\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d76e38-0e55-4e95-bcf5-87ab385cf5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    \"dbname\": \"postgres\", # use your actual database name (defaut = \"postgres\")\n",
    "    \"user\": \"username\", # use your actual database username (defaut = \"postgres\")\n",
    "    \"password\": \"********\", # use your actual database password\n",
    "    \"host\": \"localhost\",  # for local server database\n",
    "    \"port\": \"5432\" # use your actual database port (defaut = \"5432\")\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Establish connection to PostgreSQL database\n",
    "    conn = psycopg2.connect(**db_params)\n",
    "    \n",
    "    # Query the entire temperature_sensor_recording table\n",
    "    query = \"SELECT * FROM temperature_sensor_recording\"\n",
    "    \n",
    "    # Read the query results directly into a pandas DataFrame\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "    \n",
    "    # Display basic information about the DataFrame\n",
    "    print(\"Data loaded successfully!\")\n",
    "    print(f\"\\nShape: {df.shape}\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "except psycopg2.Error as e:\n",
    "    print(f\"Database error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e5d5c4-7f28-4ad2-8d23-54e5d88e6cea",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f53a60",
   "metadata": {},
   "source": [
    "Indicators:\n",
    "1. start_time: starting hour of the row\n",
    "2. availability_status: a flag that marks wheather there are 3 or more temperature data in an hour \n",
    "\n",
    "Features:\n",
    "1. num_data: number of temperature data in an hour\n",
    "2. sampling_rate: number of temperature data in a minute\n",
    "3. range: difference between maximum and minimum recorded temp data in an hour\n",
    "4. standard_deviation: standard deviation of recorded temp data in an hour\n",
    "5. covar:  standard deviation/mean of recorded temp data in an hour\n",
    "6. max_rate: maximum absolute temperature change rate in a minute\n",
    "7. min_rate: minimum absolute temperature change rate in a minute\n",
    "8. rate_range: max_rate - min_rate\n",
    "9. osc_freq: the number of temperature rate direction change in a minute\n",
    "10. var_score: percentage of anomalous temperature change in an hour\n",
    "11. hour_index: index of start_time hour in UTC+7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845763e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDITED\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "df = raw_data\n",
    "\n",
    "logger.info(f\"Loaded {len(df)} rows from CSV\")\n",
    "\n",
    "# Convert time_received to datetime and round down to minute\n",
    "logger.info(\"Processing timestamps...\")\n",
    "df['time_received'] = pd.to_datetime(df['time_received'])\n",
    "df['start_time'] = df['time_received'].dt.floor('T')  # Round down to minute\n",
    "\n",
    "# Use temp_n as temperature\n",
    "temp_n = \"temp_1\" # choose the \"temp_1\" raw data column instead of the \"temp_2\"\n",
    "df['temp'] = df[temp_n] # create another column that consists of the actual processed \"temp_1\" data\n",
    "\n",
    "# Remove rows with null temperature\n",
    "df = df[df['temp'].notna()].copy()   # delete rows with invalid NaN values \n",
    "logger.info(f\"After removing null temperatures: {len(df)} rows\") # print current valid rows\n",
    "\n",
    "# Create hour_start column (truncate to hour)\n",
    "df['hour_start'] = df['start_time'].dt.floor('H')  # create another column \"hour_start\" consisting of the hour-rounded \"start_time\" \n",
    "\n",
    "# Get date range\n",
    "min_date = df['start_time'].min().date()   # first date in the dataset\n",
    "max_date = df['start_time'].max().date()   # last date in the dataset\n",
    "logger.info(f\"Data range: {min_date} to {max_date}\")\n",
    "\n",
    "# Generate all dates and hours\n",
    "logger.info(\"Generating complete date-hour grid...\")\n",
    "all_results = []  # create an empty list \"all_results\" to store the extracted features (in dictionaries)\n",
    "\n",
    "current_date = datetime.combine(min_date, datetime.min.time())  # create starting date as min_date plus time (00:00:00) as the very beginning of the date\n",
    "end_date = datetime.combine(max_date, datetime.min.time()) + timedelta(days=1)  # create the farthest datetime (max_date + 1, 00:00:00)\n",
    "\n",
    "total_days = (end_date - current_date).days   # calculate the total number of days in the dataset\n",
    "processed_days = 0\n",
    "\n",
    "while current_date < end_date:\n",
    "    # Process all 24 hours for this date\n",
    "    for hour in range(24):  # from 0 to 23\n",
    "        hour_start = current_date + timedelta(hours=hour) # iterate through hours of the current_date \n",
    "        \n",
    "        # Filter data for this hour\n",
    "        hour_data = df[df['hour_start'] == hour_start].copy() # take all temperature recording rows within this hour as a new df (hour_data)\n",
    "        num_data = len(hour_data) # define num_data as the number of data rows in an hour\n",
    "\n",
    "        if num_data < 3:   # if there are less than 3 temperature reecordings within this hour:\n",
    "            # fill every feature with 0\n",
    "            all_results.append({\n",
    "                'num_data': 0,\n",
    "                'sampling_rate': 0.0,\n",
    "                'start_time': hour_start,\n",
    "                'availability_status': 0,\n",
    "                'range': 0.0,\n",
    "                'standard_deviation': 0.0,\n",
    "                'covar': 0.0,\n",
    "                'max_rate': 0.0,\n",
    "                'rate_range': 0.0,\n",
    "                'osc_freq': 0.0,\n",
    "                'var_score': 0.0,\n",
    "                'hour_index': (hour_start.hour + 7) % 24\n",
    "            })\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            availability_status = 1 # set availability_status to 1 if num_data >=3\n",
    "            \n",
    "            # Sort by time\n",
    "            hour_data = hour_data.sort_values('start_time') # re-arrange hour_data rows based on \"start_time\" values\n",
    "            temps = hour_data['temp'].values  # create numpy array of temperature readings\n",
    "            times = hour_data['start_time'].values # create numpy array of start times\n",
    "            \n",
    "            # Basic statistics\n",
    "            temp_range = float(np.max(temps) - np.min(temps)) # find the difference between max and min temperatures within the hour\n",
    "            std_dev = float(np.std(temps, ddof=1)) # hourly temperature standard deviation \n",
    "            mean_temp = float(np.mean(temps)) # average temp data in this hour\n",
    "            covar = (std_dev / abs(mean_temp)) if mean_temp != 0 else 0.0 # coefficient of variation\n",
    "            \n",
    "            \n",
    "            rates = []\n",
    "            abnormal_count = 0\n",
    "            \n",
    "            for i in range(len(temps) - 1):\n",
    "                temp_diff = abs(temps[i+1] - temps[i])\n",
    "                time_diff_minutes = (times[i+1] - times[i]) / np.timedelta64(1, 'm')\n",
    "                if time_diff_minutes > 0:\n",
    "                    rate = temp_diff / time_diff_minutes  # absolute temperature change rate in a minute\n",
    "                    rates.append(rate) # store temperature rates (change per minute)\n",
    "\n",
    "                    threshold = 0.031 # celsius/minute\n",
    "                    if rate > threshold:\n",
    "                        abnormal_count += 1\n",
    "            \n",
    "            max_rate = float(max(rates)) if rates else 0.0 # maximum absolute temperature change rate in a minute\n",
    "            min_rate = float(min(rates)) if rates else 0.0 # minimum absolute temperature change rate in a minute\n",
    "            rate_range = max_rate - min_rate # largest temperature change rate difference in an hour \n",
    "            \n",
    "            var_score = float(abnormal_count / len(rates)) if rates else 0.0 # calculate var_score (variability score), Threshold: 0.031°C/minute\n",
    "            \n",
    "            # Calculate oscillation frequency\n",
    "            directions = []\n",
    "            for i in range(len(temps) - 1):\n",
    "                if temps[i+1] > temps[i]:\n",
    "                    directions.append(1)\n",
    "                elif temps[i+1] < temps[i]:\n",
    "                    directions.append(-1)\n",
    "            \n",
    "            direction_changes = 0\n",
    "            for i in range(len(directions) - 1):\n",
    "                if directions[i] != directions[i+1]:\n",
    "                    direction_changes += 1\n",
    "            \n",
    "            osc_freq = float((1 + direction_changes) / 60.0) if directions else 0.0\n",
    "            \n",
    "            # Replace any NaN or inf with 0\n",
    "            std_dev = 0.0 if np.isnan(std_dev) or np.isinf(std_dev) else std_dev\n",
    "            covar = 0.0 if np.isnan(covar) or np.isinf(covar) else covar\n",
    "            max_rate = 0.0 if np.isnan(max_rate) or np.isinf(max_rate) else max_rate\n",
    "            rate_range = 0.0 if np.isnan(rate_range) or np.isinf(rate_range) else rate_range\n",
    "            var_score = 0.0 if np.isnan(var_score) or np.isinf(var_score) else var_score\n",
    "            osc_freq = 0.0 if np.isnan(osc_freq) or np.isinf(osc_freq) else osc_freq\n",
    "            \n",
    "            all_results.append({\n",
    "                'num_data': num_data,\n",
    "                'sampling_rate': num_data / 60.0,\n",
    "                'start_time': hour_start,\n",
    "                'availability_status': availability_status,\n",
    "                'range': temp_range,\n",
    "                'standard_deviation': std_dev,\n",
    "                'covar': covar,\n",
    "                'max_rate': max_rate,\n",
    "                'rate_range': rate_range,\n",
    "                'osc_freq': osc_freq,\n",
    "                'var_score': var_score,\n",
    "                'hour_index': (hour_start.hour + 7) % 24\n",
    "            })\n",
    "    \n",
    "    # Progress logging\n",
    "    processed_days += 1\n",
    "    if processed_days % 10 == 0:\n",
    "        logger.info(f\"Progress: {processed_days}/{total_days} days processed\")\n",
    "    \n",
    "    # Move to next date\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "logger.info(f\"Total days processed: {processed_days}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "logger.info(\"Creating output DataFrame...\")\n",
    "result_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Ensure all columns are present and in correct order\n",
    "output_columns = [\n",
    "    'start_time', 'hour_index', 'num_data', 'sampling_rate', 'availability_status',\n",
    "    'range', 'standard_deviation', 'covar', 'max_rate', 'rate_range',\n",
    "    'osc_freq', 'var_score' \n",
    "]\n",
    "\n",
    "result_df = result_df[output_columns]\n",
    "\n",
    "# Replace any remaining NaN with 0\n",
    "result_df = result_df.fillna(0)\n",
    "\n",
    "\n",
    "# Show statistics\n",
    "logger.info(\"=\" * 60)\n",
    "logger.info(\"PROCESSING COMPLETED!\")\n",
    "logger.info(f\"Total hourly records: {len(result_df)}\")\n",
    "logger.info(f\"Records with availability_status=1: {result_df['availability_status'].sum()}\")\n",
    "logger.info(f\"Records with availability_status=0: {len(result_df) - result_df['availability_status'].sum()}\")\n",
    "logger.info(f\"Average sampling rate: {result_df['sampling_rate'].mean():.3f}\")\n",
    "logger.info(f\"Average var_score: {result_df['var_score'].mean():.3f}\")\n",
    "logger.info(\"=\" * 60)\n",
    "\n",
    "# Show sample results\n",
    "logger.info(\"\\nValid rows:\")\n",
    "df_available = result_df[result_df['availability_status'] == 1].copy()\n",
    "df_available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e005cad-a3a2-43ec-94e7-d7f30c54d293",
   "metadata": {},
   "source": [
    "## Feature correlation matrix plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af802aa",
   "metadata": {},
   "source": [
    "plot the correlation coefficient between each feature pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c93e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Filter for availability_status == 1\n",
    "logger.info(f\"Records with availability_status=1: {len(df_available)}\")\n",
    "logger.info(f\"Percentage available: {len(df_available)/len(result_df)*100:.2f}%\")\n",
    "\n",
    "# Define features for correlation analysis\n",
    "feature_columns = [\n",
    "    \"num_data\", \"sampling_rate\", \"range\", \"standard_deviation\", \n",
    "    \"covar\", \"max_rate\", \"rate_range\", \"osc_freq\", \"var_score\", \"hour_index\"\n",
    "]\n",
    "\n",
    "# Extract feature subset\n",
    "X = df_available[feature_columns].copy()\n",
    "\n",
    "\n",
    "# Create correlation matrix\n",
    "correlation_matrix = X.corr()\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True,  # Show correlation values\n",
    "            cmap='coolwarm',  # Color scheme\n",
    "            center=0,  # Center colormap at 0\n",
    "            fmt='.2f',  # Format numbers to 2 decimal places\n",
    "            square=True,  # Make cells square-shaped\n",
    "            linewidths=0.5,  # Add gridlines\n",
    "            cbar_kws={\"shrink\": 0.8})  # Adjust colorbar size\n",
    "\n",
    "plt.title('Correlation Matrix of Features', fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print strongest correlations\n",
    "logger.info(\"\\nStrongest correlations (absolute value > 0.5):\")\n",
    "correlation_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_value) > 0.5:\n",
    "            correlation_pairs.append((\n",
    "                correlation_matrix.columns[i],\n",
    "                correlation_matrix.columns[j],\n",
    "                corr_value\n",
    "            ))\n",
    "\n",
    "for feat1, feat2, corr in sorted(correlation_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
    "    logger.info(f\"{feat1} <-> {feat2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b83dfbf-5b40-463d-a0fb-1b968caaccbb",
   "metadata": {},
   "source": [
    "## Isolation Forest outlier detection\n",
    "unique features with low correlation coefficients: sampling_rate, rate_range, var_score, hour_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eba84f8-9eeb-45b6-b421-55ef98c8a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "unique_features = [\"sampling_rate\", \"rate_range\", \"var_score\", \"hour_index\"]\n",
    "df_features = df_available[unique_features].copy() # df with only available_status == 1, with unique feature columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_features_scaled = scaler.fit_transform(df_features)\n",
    "\n",
    "# Initialize Isolation Forest\n",
    "model = IsolationForest(\n",
    "    n_estimators=400,\n",
    "    max_samples='auto',\n",
    "    contamination=0.1,  # Expect 10% outliers (adjust based on your domain knowledge)\n",
    "    random_state=42,  # For reproducibility\n",
    "    n_jobs=-1  # Use all CPU cores for faster training\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "model.fit(df_features_scaled)\n",
    "\n",
    "# Get predictions\n",
    "# -1 = outlier, 1 = inlier\n",
    "prediction_labels = model.predict(df_features_scaled)\n",
    "pred_labels = []\n",
    "\n",
    "for i in range (len (prediction_labels)):\n",
    "    if prediction_labels[i] == -1:\n",
    "        pred_labels.append(0)\n",
    "    else:\n",
    "        pred_labels.append(1)\n",
    "        \n",
    "# Get anomaly scores (more negative = more anomalous)\n",
    "anomaly_scores = model.decision_function(df_features_scaled)\n",
    "\n",
    "min_score = anomaly_scores.min()\n",
    "max_score = anomaly_scores.max()\n",
    "\n",
    "normalized_scores = (anomaly_scores - min_score) / (max_score - min_score) # normalize anomaly_scores\n",
    "\n",
    "anomaly_scores = normalized_scores.copy()\n",
    "\n",
    "# Add to original dataframe\n",
    "df_available[\"pred_labels\"] = pred_labels\n",
    "df_available[\"anomaly_score\"] = anomaly_scores\n",
    "\n",
    "# Results\n",
    "df_available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c60b1df-1c1b-4f3a-9f07-dbbf3a73fc02",
   "metadata": {},
   "source": [
    "## Hourly temperature sensor diagnosis finalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dab0722",
   "metadata": {},
   "source": [
    "creating a complete hourly diagnosis result dataframe (df_hourly) that contains the extracted features, anomaly score, and hourly health status based on the isolation forest outlier detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c3c340-4c0a-482e-b549-68909eddcafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Starting merge process...\")\n",
    "logger.info(f\"df shape: {result_df.shape}\")\n",
    "logger.info(f\"df_available shape: {df_available.shape}\")\n",
    "\n",
    "# Create a copy of df to avoid modifying original\n",
    "df_hourly = result_df.copy()\n",
    "\n",
    "# Verify that df_available is a subset of df\n",
    "n_available = (df_hourly['availability_status'] == 1).sum()\n",
    "logger.info(f\"Rows with availability_status == 1 in df_hourly: {n_available}\")\n",
    "logger.info(f\"Rows in df_available: {len(df_available)}\")\n",
    "\n",
    "if n_available != len(df_available):\n",
    "    logger.warning(f\"Mismatch detected! Expected {n_available} but got {len(df_available)}\")\n",
    "\n",
    "# Step 1: Prepare df_available for merging\n",
    "# Select only the columns we need and rename pred_labels to health_status\n",
    "df_available_merge = df_available[['start_time', 'pred_labels', 'anomaly_score']].copy()\n",
    "df_available_merge = df_available_merge.rename(columns={'pred_labels': 'health_status'})\n",
    "\n",
    "logger.info(\"\\nPrepared df_available for merging:\")\n",
    "logger.info(f\"Columns: {df_available_merge.columns.tolist()}\")\n",
    "logger.info(f\"health_status value counts:\\n{df_available_merge['health_status'].value_counts()}\")\n",
    "\n",
    "# Step 2: Merge df_hourly with df_available on start_time (left join)\n",
    "logger.info(\"\\nPerforming left merge on 'start_time'...\")\n",
    "df_hourly = df_hourly.merge(\n",
    "    df_available_merge,\n",
    "    on='start_time',\n",
    "    how='left',\n",
    "    suffixes=('', '_from_available')\n",
    ")\n",
    "\n",
    "logger.info(f\"After merge, df_hourly shape: {df_hourly.shape}\")\n",
    "\n",
    "# Step 3: Fill NaN values based on availability_status\n",
    "logger.info(\"\\nFilling NaN values based on availability_status...\")\n",
    "\n",
    "# For rows where availability_status == 0, fill with defaults\n",
    "mask_unavailable = df_hourly['availability_status'] == 0\n",
    "n_unavailable = mask_unavailable.sum()\n",
    "\n",
    "df_hourly.loc[mask_unavailable, 'health_status'] = 1\n",
    "df_hourly.loc[mask_unavailable, 'anomaly_score'] = 0.0\n",
    "\n",
    "logger.info(f\"Filled {n_unavailable} rows with availability_status == 0\")\n",
    "logger.info(f\"  health_status = 1 (healthy)\")\n",
    "logger.info(f\"  anomaly_score = 0.0\")\n",
    "\n",
    "# Step 4: Verify no NaN values remain in health_status and anomaly_score\n",
    "nan_health = df_hourly['health_status'].isna().sum()\n",
    "nan_anomaly = df_hourly['anomaly_score'].isna().sum()\n",
    "\n",
    "if nan_health > 0 or nan_anomaly > 0:\n",
    "    logger.warning(f\"\\nWarning: Found unexpected NaN values!\")\n",
    "    logger.warning(f\"  NaN in health_status: {nan_health}\")\n",
    "    logger.warning(f\"  NaN in anomaly_score: {nan_anomaly}\")\n",
    "    \n",
    "    # Show which rows have NaN\n",
    "    if nan_health > 0:\n",
    "        logger.warning(\"\\nRows with NaN health_status:\")\n",
    "        print(df_hourly[df_hourly['health_status'].isna()][['start_time', 'availability_status']].head(10))\n",
    "else:\n",
    "    logger.info(\"\\n✓ No NaN values found in health_status or anomaly_score\")\n",
    "\n",
    "# Step 5: Ensure correct data types\n",
    "df_hourly['health_status'] = df_hourly['health_status'].astype(int)\n",
    "df_hourly['anomaly_score'] = df_hourly['anomaly_score'].astype(float)\n",
    "\n",
    "# Step 6: Print summary statistics\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"MERGE SUMMARY\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"\\nTotal rows: {len(df_hourly)}\")\n",
    "logger.info(f\"\\nAvailability Status distribution:\")\n",
    "logger.info(df_hourly['availability_status'].value_counts().to_string())\n",
    "\n",
    "logger.info(f\"\\nHealth Status distribution:\")\n",
    "health_counts = df_hourly['health_status'].value_counts().sort_index()\n",
    "for status, count in health_counts.items():\n",
    "    pct = count / len(df_hourly) * 100\n",
    "    status_name = \"Anomaly/Unhealthy\" if status == 0 else \"Normal/Healthy\"\n",
    "    logger.info(f\"  {status} ({status_name}): {count} ({pct:.2f}%)\")\n",
    "\n",
    "logger.info(f\"\\nAnomaly Score statistics:\")\n",
    "logger.info(f\"  Min: {df_hourly['anomaly_score'].min():.4f}\")\n",
    "logger.info(f\"  Max: {df_hourly['anomaly_score'].max():.4f}\")\n",
    "logger.info(f\"  Mean: {df_hourly['anomaly_score'].mean():.4f}\")\n",
    "logger.info(f\"  Median: {df_hourly['anomaly_score'].median():.4f}\")\n",
    "\n",
    "# Cross-tabulation: availability_status vs health_status\n",
    "logger.info(f\"\\nCross-tabulation (availability_status vs health_status):\")\n",
    "crosstab = pd.crosstab(\n",
    "    df_hourly['availability_status'], \n",
    "    df_hourly['health_status'],\n",
    "    margins=True,\n",
    "    margins_name='Total'\n",
    ")\n",
    "print(crosstab)\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"MERGE COMPLETED SUCCESSFULLY!\")\n",
    "logger.info(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dc3626-2883-4ac8-ae0f-2d049ac79ef5",
   "metadata": {},
   "source": [
    "## Daily temperature sensor health scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774a2a98",
   "metadata": {},
   "source": [
    "calculation of: \n",
    "1. transmission health score (transmission_hs): the percentage of availaibility_status in each day\n",
    "2. sensor health score (sensor_hs): the percentage of health_status in each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b1bcc-c7aa-4311-ac59-51b425a61094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(f\"Loaded {len(df_hourly)} hourly records\")\n",
    "\n",
    "# Parse start_time as datetime\n",
    "logger.info(\"Parsing start_time as datetime...\")\n",
    "df_hourly['start_time'] = pd.to_datetime(df_hourly['start_time'])\n",
    "\n",
    "# Extract date from start_time\n",
    "df_hourly['date'] = df_hourly['start_time'].dt.date\n",
    "\n",
    "# Get date range\n",
    "min_date = df_hourly['date'].min()   # oldest date in df_hourly ['date']\n",
    "max_date = df_hourly['date'].max()   # latest date in df_hourly ['date']\n",
    "logger.info(f\"Date range: {min_date} to {max_date}\")\n",
    "\n",
    "total_days = (max_date - min_date).days + 1\n",
    "logger.info(f\"Total days to process: {total_days}\")\n",
    "\n",
    "# Initialize list to store daily results\n",
    "daily_results = []\n",
    "\n",
    "# Process day by day\n",
    "logger.info(\"\\nProcessing daily summaries...\")\n",
    "current_date = min_date\n",
    "processed_days = 0\n",
    "\n",
    "while current_date <= max_date: # iterate througgh dates in df_hourly\n",
    "    # Filter data for this specific day\n",
    "    day_data = df_hourly[df_hourly['date'] == current_date] # take only the hourly data in the current_date \n",
    "    \n",
    "    # Count hours in this day\n",
    "    n_hours = len(day_data)\n",
    "    \n",
    "    # Count availability_status == 1\n",
    "    n_available = (day_data['availability_status'] == 1).sum()\n",
    "    \n",
    "    # Count health_status == 1\n",
    "    n_healthy = (day_data['health_status'] == 1).sum()\n",
    "    \n",
    "    # Calculate percentages (always divide by 24)\n",
    "    transmission_hs = int((n_available / 24) * 100)\n",
    "    sensor_hs = int((n_healthy / 24) * 100)\n",
    "    \n",
    "    # Store result\n",
    "    daily_results.append({\n",
    "        'date': current_date.strftime('%Y-%m-%d'),\n",
    "        'transmission_hs': transmission_hs,\n",
    "        'sensor_hs': sensor_hs\n",
    "    })\n",
    "    \n",
    "    # Log if day has incomplete data\n",
    "    if n_hours != 24:\n",
    "        logger.warning(f\"  {current_date}: Only {n_hours}/24 hours found!\")\n",
    "    \n",
    "    # Progress logging\n",
    "    processed_days += 1\n",
    "    if processed_days % 30 == 0:\n",
    "        logger.info(f\"Progress: {processed_days}/{total_days} days processed\")\n",
    "    \n",
    "    # Move to next day\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "logger.info(f\"Completed processing {processed_days} days\")\n",
    "\n",
    "# Create daily DataFrame\n",
    "logger.info(\"\\nCreating daily diagnosis DataFrame...\")\n",
    "df_daily = pd.DataFrame(daily_results)\n",
    "\n",
    "# Ensure correct data types\n",
    "df_daily['transmission_hs'] = df_daily['transmission_hs'].astype(int)\n",
    "df_daily['sensor_hs'] = df_daily['sensor_hs'].astype(int)\n",
    "\n",
    "# Print summary statistics\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"DAILY DIAGNOSIS SUMMARY\")\n",
    "logger.info(\"=\"*80)\n",
    "logger.info(f\"\\nTotal days: {len(df_daily)}\")\n",
    "\n",
    "logger.info(f\"\\nTransmission Health Status (transmission_hs):\")\n",
    "logger.info(f\"  Min: {df_daily['transmission_hs'].min()}%\")\n",
    "logger.info(f\"  Max: {df_daily['transmission_hs'].max()}%\")\n",
    "logger.info(f\"  Mean: {df_daily['transmission_hs'].mean():.2f}%\")\n",
    "logger.info(f\"  Median: {df_daily['transmission_hs'].median()}%\")\n",
    "\n",
    "logger.info(f\"\\nSensor Health Status (sensor_hs):\")\n",
    "logger.info(f\"  Min: {df_daily['sensor_hs'].min()}%\")\n",
    "logger.info(f\"  Max: {df_daily['sensor_hs'].max()}%\")\n",
    "logger.info(f\"  Mean: {df_daily['sensor_hs'].mean():.2f}%\")\n",
    "logger.info(f\"  Median: {df_daily['sensor_hs'].median()}%\")\n",
    "\n",
    "# Days with perfect transmission\n",
    "perfect_transmission = (df_daily['transmission_hs'] == 100).sum()\n",
    "logger.info(f\"\\nDays with 100% transmission: {perfect_transmission} ({perfect_transmission/len(df_daily)*100:.2f}%)\")\n",
    "\n",
    "# Days with perfect sensor health\n",
    "perfect_sensor = (df_daily['sensor_hs'] == 100).sum()\n",
    "logger.info(f\"Days with 100% sensor health: {perfect_sensor} ({perfect_sensor/len(df_daily)*100:.2f}%)\")\n",
    "\n",
    "# Days with both perfect\n",
    "both_perfect = ((df_daily['transmission_hs'] == 100) & (df_daily['sensor_hs'] == 100)).sum()\n",
    "logger.info(f\"Days with both 100%: {both_perfect} ({both_perfect/len(df_daily)*100:.2f}%)\")\n",
    "\n",
    "# Distribution bins\n",
    "logger.info(f\"\\nTransmission Health Distribution:\")\n",
    "transmission_bins = pd.cut(df_daily['transmission_hs'], bins=[0, 25, 50, 75, 100], \n",
    "                            labels=['0-25%', '26-50%', '51-75%', '76-100%'], \n",
    "                            include_lowest=True)\n",
    "logger.info(transmission_bins.value_counts().sort_index().to_string())\n",
    "\n",
    "logger.info(f\"\\nSensor Health Distribution:\")\n",
    "sensor_bins = pd.cut(df_daily['sensor_hs'], bins=[0, 25, 50, 75, 100], \n",
    "                     labels=['0-25%', '26-50%', '51-75%', '76-100%'], \n",
    "                     include_lowest=True)\n",
    "logger.info(sensor_bins.value_counts().sort_index().to_string())\n",
    "\n",
    "# Show sample data\n",
    "logger.info(f\"\\nFirst 10 days:\")\n",
    "print(df_daily.head(10).to_string(index=False))\n",
    "\n",
    "logger.info(f\"\\nLast 10 days:\")\n",
    "print(df_daily.tail(10).to_string(index=False))\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*80)\n",
    "logger.info(\"DAILY DIAGNOSIS CREATION COMPLETED!\")\n",
    "logger.info(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb8ab1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11df42ae",
   "metadata": {},
   "source": [
    "## Upload df_hourly and df_daily health assesment results into postgres tables\n",
    "\n",
    "df_hourly ==> tempsensor_hourly_healthstatus\n",
    "\n",
    "df_daily ==> tempsensor_daily_healthscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc482e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    \"dbname\": \"postgres\",\n",
    "    \"user\": \"Username\",\n",
    "    \"password\": \"******\",\n",
    "    \"host\": \"localhost\",  \n",
    "    \"port\": \"5432\"\n",
    "}\n",
    "\n",
    "# Assuming you have df_hourly and df_daily DataFrames already created\n",
    "# df_hourly = your hourly data\n",
    "# df_daily = your daily data\n",
    "\n",
    "try:\n",
    "    # Create SQLAlchemy engine for easier DataFrame upload\n",
    "    connection_string = f\"postgresql://{db_params['user']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/{db_params['dbname']}\"\n",
    "    engine = create_engine(connection_string)\n",
    "    \n",
    "    # Upload df_hourly to tempsensor_hourly_healthstatus table\n",
    "    df_hourly.to_sql(\n",
    "        name='tempsensor_hourly_healthstatus',\n",
    "        con=engine,\n",
    "        if_exists='replace',  # Options: 'fail', 'replace', 'append'\n",
    "        index=False  # Don't write DataFrame index as a column\n",
    "    )\n",
    "    print(\"✓ df_hourly uploaded to tempsensor_hourly_healthstatus\")\n",
    "    \n",
    "    # Upload df_daily to tempsensor_daily_healthscore table\n",
    "    df_daily.to_sql(\n",
    "        name='tempsensor_daily_healthscore',\n",
    "        con=engine,\n",
    "        if_exists='replace',  # Options: 'fail', 'replace', 'append'\n",
    "        index=False  # Don't write DataFrame index as a column\n",
    "    )\n",
    "    print(\"✓ df_daily uploaded to tempsensor_daily_healthscore\")\n",
    "    \n",
    "    # Close the engine\n",
    "    engine.dispose()\n",
    "    \n",
    "    print(\"\\nBoth DataFrames uploaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
